{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "a59a1f68-c41f-4c68-84a0-4f21f81e5a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Simulator.Tasks.Task import Task\n",
    "from Simulator.Tasks.Requirements import Requirements\n",
    "from Simulator.Machines.Resources import Resources\n",
    "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "\n",
    "import importlib\n",
    "from gym import Env\n",
    "from gym import spaces\n",
    "\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import Simulator.ResSim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "0dc8b8b5-39d4-43d3-b44c-9c8f99b10ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simulator.ResSim  = importlib.reload(Simulator.ResSim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "4f047312-5eb0-4cea-90f5-aff4d5a3c08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "fe3f848c-2c56-4698-84dd-73b00e2ac241",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveResults(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages, 2 for debug messages\n",
    "    \"\"\"\n",
    "    def __init__(self, list_avg_reward_save):\n",
    "        super(SaveResults, self).__init__(1)\n",
    "        self.avg_reward = list_avg_reward_save\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # print(\"HERE\")\n",
    "        \n",
    "        x, y = ts2xy(load_results(\"log/\"), \"timesteps\")\n",
    "        \n",
    "        if len(x) > 0:\n",
    "            if len(x) > 20:\n",
    "                self.avg_reward.append((self.num_timesteps, np.mean(np.asarray(y[-20:]))))\n",
    "            else:\n",
    "                self.avg_reward.append((self.num_timesteps, y[-1]))\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "6cc56bc0-2458-4dbb-b21d-3089415ec6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sim():\n",
    "    \"\"\" Method to create a sim and return it to keep the enviroment code clean\"\"\"\n",
    "    \n",
    "    # Create Traffic\n",
    "    \n",
    "    test_traffic = []\n",
    "    for x in range(0, 500):\n",
    "        if np.random.random() > 0.5:\n",
    "            # test_traffic.append((x, Task(Requirements(), 10)))\n",
    "            test_traffic.append((x, Task(Requirements(), 10)))\n",
    "        elif np.random.random() > 0.5:\n",
    "            test_traffic.append((x, Task(Requirements(cpus =2), 15)))\n",
    "            # test_traffic.append((x, Task(Requirements(), 10)))\n",
    "        \n",
    "    sim = Simulator.ResSim.ResSim(test_traffic, lambda x: int(x.cpus == 2), [Resources(), Resources(), Resources(cpus=2, power_level = 2)], 2)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "6728cca9-e3cd-4b01-888f-42ac2c3e1415",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimEnv(Env):\n",
    "    def __init__(self):\n",
    "        \"\"\" Init the sim enviroment \"\"\"\n",
    "        super(SimEnv, self).__init__()\n",
    "        self.sim = create_sim()\n",
    "        # self.action_space = \n",
    "        # self.observation_space = len(self.sim.getState())\n",
    "        self.action_space = spaces.Discrete(self.sim.getActionDem())\n",
    "        self.observation_space = spaces.Box(low=-1.0, high=2.0,\n",
    "                                        shape=(len(self.sim.getState()), ), dtype=np.float64)\n",
    "        self.results = []\n",
    "        self.avg_results = []\n",
    "        self.episode_return = []\n",
    "        self.episodes = 0 \n",
    "        self.episode_avg_task_per_timestep = []\n",
    "        self.avg_episode_avg_task_per_timestep = []\n",
    "        self.episode_curTime = []\n",
    "        self.avg_episode_curTime = []\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\" Take the action \"\"\"\n",
    "        reward = self.sim.doAction(action)\n",
    "        # print(action)\n",
    "        info = {}\n",
    "        self.results.append(reward)            \n",
    "            \n",
    "        if self.sim.done:\n",
    "            num_completed = len(self.sim.sysLogger.completed_tasks)\n",
    "            self.episode_avg_task_per_timestep.append(num_completed / self.sim.curTime)\n",
    "            self.episode_curTime.append(self.sim.curTime)\n",
    "            \n",
    "            self.episode_return.append(np.sum(self.results))\n",
    "            if len(self.episode_return) > 20:\n",
    "                self.avg_results.append(np.mean(self.episode_return[-20:]))\n",
    "                self.avg_episode_curTime.append(np.mean(self.episode_curTime[-20]))\n",
    "                self.avg_episode_avg_task_per_timestep.append(np.mean(self.episode_avg_task_per_timestep[-20]))\n",
    "            else:\n",
    "                self.avg_results.append(np.sum(self.results))\n",
    "                self.avg_episode_avg_task_per_timestep.append(num_completed / self.sim.curTime)\n",
    "                self.avg_episode_curTime.append(self.sim.curTime)\n",
    "            self.episodes += 1\n",
    "            # print(self.sim.num_completed_tasks)\n",
    "        # if reward != -100:\n",
    "        #     print(reward)\n",
    "        return self.sim.getState(), reward, self.sim.done, info\n",
    "        # Todo\n",
    "    \n",
    "    # def render(self, action):\n",
    "    #     # Blank\n",
    "    \n",
    "    def reset(self):\n",
    "        self.results = []\n",
    "        \"\"\" Reset the enviroment\"\"\"\n",
    "        self.sim = create_sim()\n",
    "        return self.sim.getState()\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Closing\"\"\"\n",
    "        print(\"All done\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b736dc-32b3-49b5-a223-ed0e58f524a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "env = Monitor(SimEnv(), \"log/\")\n",
    "baseline = DQN(\n",
    "    \"MlpPolicy\", \n",
    "    env, \n",
    "    verbose=0, \n",
    "    buffer_size = 1000, \n",
    "    batch_size = 64,\n",
    "    gamma = 0.99,\n",
    "    train_freq= 1,\n",
    "    learning_starts = 1000,\n",
    "    target_update_interval = 100,\n",
    "    exploration_fraction = 1,\n",
    "    exploration_final_eps = 0.01,\n",
    "    exploration_initial_eps = 1,\n",
    "    learning_rate = 0.002,\n",
    "    policy_kwargs= {'net_arch': [400, 400]}\n",
    ")\n",
    "avg_reward_baseline = []\n",
    "baseline.learn(900000, callback = SaveResults(avg_reward_baseline))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3911fd32-3cb2-468a-a3f6-4da903b61490",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reward_baseline_2 = np.array(avg_reward_baseline)\n",
    "plt.plot(avg_reward_baseline_2[:, 0], avg_reward_baseline_2[:, 1],label=\"Standard Baseline\")\n",
    "plt.title(\"Moving Average Training Reward Return\")\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"Moving Average Return (20 Episodes)\")\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed8493c-2a6e-4664-9319-7376feaab3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(moving_average(env.results, 20))\n",
    "# plt.plot(env.results,'^',label=\"Return\")\n",
    "plt.plot(env.avg_results[:],'b',label=\"Average Return (Last 20 episodes)\")\n",
    "plt.plot(env.episode_return[:], 'r', label = \"Per Episode Returns\")\n",
    "plt.title(\"Training Reward Results\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Reward (1e6)\")\n",
    "a = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc5bab-4e08-4d3a-9189-e64c2d9df88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(env.avg_episode_avg_task_per_timestep[:])\n",
    "plt.title(\"Average ratio of number of completed task to episode length\")\n",
    "plt.ylabel(\"Ratio of number of completed tasks to episode length\")\n",
    "_ = plt.xlabel(\"Episode Number\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f4f531-8d57-4838-a946-bc670d20149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(env.episode_curTime)\n",
    "plt.title(\"Number of timesteps per episode\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Number of timesteps within the episode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450ac5cb-5fd8-46ac-b96c-4f8a11dc448a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c72fb66-6f01-4b77-bde7-9ecf782ca2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
